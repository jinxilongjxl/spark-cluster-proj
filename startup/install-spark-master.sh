#!/bin/bash
set -e  # å‡ºé”™å³é€€å‡ºï¼Œå¢žå¼ºå¥å£®æ€§
exec > /var/log/spark-master-install.log 2>&1

echo "=============================="
echo "ðŸš€ Starting Spark Master Installation"
echo "=============================="

# 1. å®‰è£…ä¾èµ–ï¼ˆJavaã€SSHå·¥å…·ï¼‰
echo "ðŸ”§ Step 1: Installing OpenJDK 11 + SSH ä¾èµ–..."
apt update -y
apt install -y openjdk-11-jdk openssh-server pdsh
echo "âœ… Java + SSH ä¾èµ–å®‰è£…å®Œæˆ: $(java -version 2>&1 | head -1)"

# 2. åˆ›å»º spark ç”¨æˆ·
echo "ðŸ‘¤ Step 2: Creating spark user..."
id spark &>/dev/null || useradd -m -s /bin/bash spark
echo "âœ… Spark user created"

# 3. å®‰è£… Spark
echo "ðŸ“¦ Step 3: Installing Spark 3.4.1..."
SPARK_HOME="/home/spark/spark"
if [ ! -d "$SPARK_HOME" ]; then
  su - spark -c "
    cd /home/spark
    echo 'ðŸ“¥ Downloading Spark...'
    wget -q https://dlcdn.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz || exit 1
    echo 'ðŸ” Extracting Spark...'
    tar -xzf spark-3.4.1-bin-hadoop3.tgz || exit 1
    mv spark-3.4.1-bin-hadoop3 spark
    echo 'âœ… Spark installed successfully.'
  "
else
  echo "âœ… Spark already installed"
fi

# 4. é…ç½®çŽ¯å¢ƒå˜é‡ï¼ˆå¹¶ä¿®å¤æƒé™ï¼‰
echo "âš™ï¸ Step 4: Configuring environment variables..."
cat > /home/spark/.bashrc << 'EOF'
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export SPARK_HOME=/home/spark/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
export PDSH_RCMD_TYPE=ssh
EOF
chown spark:spark /home/spark/.bashrc  # ä¿®å¤æ‰€æœ‰è€…
echo "âœ… .bashrc configured"

# 5. é…ç½® spark-env.shï¼ˆå¹¶ä¿®å¤æƒé™ï¼‰
echo "âš™ï¸ Step 5: Configuring spark-env.sh..."
cat > $SPARK_HOME/conf/spark-env.sh << 'EOF'
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export SPARK_MASTER_HOST=$(hostname -i)
export SPARK_MASTER_PORT=7077
export SPARK_WORKER_MEMORY=2g
export SPARK_WORKER_CORES=1
EOF
chown spark:spark $SPARK_HOME/conf/spark-env.sh  # ä¿®å¤æƒé™
echo "âœ… spark-env.sh updated"

# 6. ç”Ÿæˆ Master è‡ªèº«çš„ SSH å¯†é’¥ï¼ˆç”¨äºŽ localhost å…å¯† + åˆ†å‘åˆ° Workerï¼‰
echo "ðŸ”‘ Step 6: Generating SSH key for cluster communication..."
su - spark -c "
  echo 'Generating SSH key pair...'
  ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa -q  # æ— å¯†ç ç”Ÿæˆå¯†é’¥
  # æ£€æŸ¥å…¬é’¥æ˜¯å¦ç”ŸæˆæˆåŠŸ
  if [ ! -f ~/.ssh/id_rsa.pub ]; then
    echo 'âŒ Failed to generate SSH public key'
    exit 1  # ç”Ÿæˆå¤±è´¥åˆ™è„šæœ¬é€€å‡ºï¼Œé¿å…åŽç»­æ­¥éª¤æ— æ•ˆ
  fi
  chmod 700 ~/.ssh
  cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
  chmod 600 ~/.ssh/authorized_keys
  echo 'âœ… SSH key generated for localhost'
"

# 7. å¯åŠ¨ Sparkï¼ˆæ·»åŠ é‡è¯•å’ŒçŠ¶æ€æ£€æŸ¥ï¼‰
echo "ðŸš€ Step 7: Starting Spark services with restart on failure..."
cat > /tmp/start-spark.sh << 'EOF'
#!/bin/bash
source /home/spark/.bashrc

# é…ç½®å‚æ•°
WORKERS=("spark-worker-1" "spark-worker-2")  # éœ€ä¸ŽTerraformä¸­workeråç§°ä¸€è‡´
PORTS=("8081")  # Worker Web UI ç«¯å£
MAX_RETRIES=5   # å•æ¬¡æ£€æµ‹æœ€å¤§å°è¯•æ¬¡æ•°
RETRY_DELAY=10  # æ¯æ¬¡å°è¯•é—´éš”10ç§’
MAX_RESTARTS=1  # æœ€å¤§é‡å¯æ¬¡æ•°

# åˆå§‹å¯åŠ¨ Spark Master
echo "Starting initial Spark Master..."
start-master.sh

# å®šä¹‰ç«¯å£æ£€æµ‹å‡½æ•°ï¼ˆå‚æ•°ï¼šworker, portï¼‰
check_port() {
  local worker=$1
  local port=$2
  local retry_count=0
  
  while [ $retry_count -lt $MAX_RETRIES ]; do
    echo "Checking $worker:$port (attempt $((retry_count+1))/$MAX_RETRIES)..."
    if nc -z $worker $port; then
      echo "$worker:$port is ready"
      return 0  # æˆåŠŸ
    fi
    retry_count=$((retry_count+1))
    sleep $RETRY_DELAY
  done
  
  # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¿”å›žå¤±è´¥
  echo "Error: $worker:$port failed after $MAX_RETRIES attempts"
  return 1
}

# ä¸»æ£€æµ‹é€»è¾‘ï¼ˆå¸¦é‡å¯æœºåˆ¶ï¼‰
restart_count=0
all_ready=0

while [ $restart_count -le $MAX_RESTARTS ]; do
  all_ready=1  # å‡è®¾æ‰€æœ‰ç«¯å£å°±ç»ª
  
  # æ£€æµ‹æ‰€æœ‰ Worker çš„ç«¯å£
  for worker in "${WORKERS[@]}"; do
    for port in "${PORTS[@]}"; do
      if ! check_port $worker $port; then
        all_ready=0  # æ ‡è®°æœ‰ç«¯å£æœªå°±ç»ª
        break  # è·³å‡ºå½“å‰ç«¯å£å¾ªçŽ¯
      fi
    done
    if [ $all_ready -eq 0 ]; then
      break  # è·³å‡ºå½“å‰ Worker å¾ªçŽ¯
    fi
  done
  
  # æ‰€æœ‰ç«¯å£å°±ç»ªï¼Œé€€å‡ºå¾ªçŽ¯
  if [ $all_ready -eq 1 ]; then
    echo "âœ… All ports are ready"
    break
  fi
  
  # æœªå°±ç»ªä¸”æœªè¾¾æœ€å¤§é‡å¯æ¬¡æ•°ï¼Œé‡å¯ Master å¹¶é‡è¯•
  if [ $restart_count -lt $MAX_RESTARTS ]; then
    restart_count=$((restart_count+1))
    echo "ðŸ”„ Restarting Spark Master (restart $restart_count/$MAX_RESTARTS)..."
    stop-master.sh
    start-master.sh
    echo "Restarted Spark Master, rechecking ports..."
  else
    # è¾¾æœ€å¤§é‡å¯æ¬¡æ•°ä»å¤±è´¥ï¼Œé€€å‡º
    echo "âŒ Failed after $MAX_RESTARTS restarts. Aborting."
    exit 1
  fi
done

echo "âœ… Spark Master service is fully ready at $(date)"
EOF

chmod +x /tmp/start-spark.sh
su - spark -c "/tmp/start-spark.sh"
echo "âœ… Spark services started with restart on failure"