#!/bin/bash
set -e
exec > /var/log/spark-worker-install.log 2>&1

echo "===== å¼€å§‹å®‰è£…Spark Worker ====="

# å…³é”®ï¼šä»Ž Terraform æ³¨å…¥çš„ metadata ä¸­èŽ·å– Master IPï¼ˆæ— éœ€æ‰‹åŠ¨è¾“å…¥ï¼‰
echo "æ­¥éª¤0ï¼šèŽ·å–MasterèŠ‚ç‚¹IP"
MASTER_IP=$(curl -s "http://metadata.google.internal/computeMetadata/v1/instance/attributes/master_ip" -H "Metadata-Flavor: Google")
if [ -z "$MASTER_IP" ]; then
  echo "âŒ æ— æ³•ä»ŽmetadataèŽ·å–Master IPï¼Œå®‰è£…å¤±è´¥"
  exit 1
fi
echo "âœ… ä»ŽmetadataèŽ·å–Master IPï¼š$MASTER_IP"

# 1. å®‰è£…ä¾èµ–
echo "æ­¥éª¤1ï¼šå®‰è£…Javaå’ŒSSH"
apt update -y
apt install -y openjdk-11-jdk openssh-server
systemctl enable --now ssh
echo "Javaç‰ˆæœ¬ï¼š$(java -version 2>&1 | head -1)"

# 2. åˆ›å»ºsparkç”¨æˆ·
echo "æ­¥éª¤2ï¼šåˆ›å»ºsparkç”¨æˆ·"
id spark &>/dev/null || useradd -m -s /bin/bash spark
echo "sparkç”¨æˆ·IDï¼š$(id -u spark)"

# 3. å®‰è£…Spark 3.5.7
echo "æ­¥éª¤3ï¼šå®‰è£…Spark 3.5.7"
SPARK_HOME="/home/spark/spark"
SPARK_TAR="spark-3.5.7-bin-hadoop3.tgz"
SPARK_URL="https://dlcdn.apache.org/spark/spark-3.5.7/$SPARK_TAR"
BACKUP_URL="https://mirrors.aliyun.com/apache/spark/spark-3.5.7/$SPARK_TAR"

if [ ! -d "$SPARK_HOME" ]; then
  su - spark -c "
    cd /home/spark
    echo 'ðŸ“¥ ä»Žä¸»æºä¸‹è½½Spark...'
    if ! wget --show-progress --retry-connrefused --waitretry=3 --read-timeout=30 --timeout=15 -t 5 $SPARK_URL; then
      echo 'âŒ ä¸»æºå¤±è´¥ï¼Œåˆ‡æ¢å¤‡ç”¨æº...'
      if ! wget --show-progress --retry-connrefused --waitretry=3 --read-timeout=30 --timeout=15 -t 5 $BACKUP_URL; then
        echo 'âŒ å¤‡ç”¨æºä¹Ÿå¤±è´¥'
        exit 1
      fi
    fi
    echo 'ï¿½è§£åŽ‹Spark...'
    if ! tar -xzf $SPARK_TAR; then
      echo 'âŒ è§£åŽ‹å¤±è´¥'
      exit 1
    fi
    mv spark-3.5.7-bin-hadoop3 spark
    rm -f $SPARK_TAR
    echo 'âœ… Sparkå®‰è£…æˆåŠŸ'
  "
else
  echo "âœ… Sparkå·²å®‰è£…"
fi
chown -R spark:spark $SPARK_HOME

# 4. é…ç½®æ°¸ä¹…çŽ¯å¢ƒå˜é‡ï¼ˆè‡ªåŠ¨æ³¨å…¥Master IPï¼‰
echo "æ­¥éª¤4ï¼šé…ç½®çŽ¯å¢ƒå˜é‡"
su - spark -c "
  cat >> ~/.bashrc << EOF
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export SPARK_HOME=/home/spark/spark
export SPARK_MASTER_HOST=$MASTER_IP  # åŠ¨æ€æ³¨å…¥Master IP
export SPARK_MASTER_PORT=7077
export SPARK_WORKER_MEMORY=2g
export SPARK_WORKER_CORES=1
export PATH=\$PATH:\$SPARK_HOME/bin:\$SPARK_HOME/sbin
EOF
  source ~/.bashrc  # å¼ºåˆ¶åŠ è½½
"

# 5. é…ç½®spark-env.shï¼ˆè‡ªåŠ¨æ³¨å…¥Master IPï¼‰
echo "æ­¥éª¤5ï¼šé…ç½®spark-env.sh"
su - spark -c "
  export SPARK_HOME=/home/spark/spark
  cp \$SPARK_HOME/conf/spark-env.sh.template \$SPARK_HOME/conf/spark-env.sh
  cat >> \$SPARK_HOME/conf/spark-env.sh << EOF
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export SPARK_MASTER_HOST=$MASTER_IP  # åŠ¨æ€æ³¨å…¥Master IP
export SPARK_MASTER_PORT=7077
export SPARK_WORKER_MEMORY=2g
export SPARK_WORKER_CORES=1
EOF
"

# 6. é…ç½®hostsï¼ˆç¡®ä¿spark-masteråŸŸåæ˜ å°„åˆ°æ­£ç¡®IPï¼‰
echo "æ­¥éª¤6ï¼šé…ç½®hostsæ˜ å°„"
echo "$MASTER_IP  spark-master" | sudo tee -a /etc/hosts
echo "âœ… hostsé…ç½®å®Œæˆï¼š$MASTER_IP spark-master"

# 7. åˆå§‹åŒ–SSHç›®å½•
echo "æ­¥éª¤7ï¼šåˆå§‹åŒ–SSHç›®å½•"
su - spark -c "
  mkdir -p ~/.ssh
  chmod 700 ~/.ssh
  touch ~/.ssh/authorized_keys
  chmod 600 ~/.ssh/authorized_keys
"

# 8. å¯åŠ¨Workerï¼ˆä½¿ç”¨åŠ¨æ€èŽ·å–çš„Master IPï¼‰
echo "æ­¥éª¤8ï¼šå¯åŠ¨Spark Worker"
su - spark -c "
  source ~/.bashrc  # å†æ¬¡åŠ è½½çŽ¯å¢ƒå˜é‡ç¡®ä¿ç”Ÿæ•ˆ
  echo 'ðŸ”— è¿žæŽ¥Masterï¼š$MASTER_IP:7077'
  /home/spark/spark/sbin/start-worker.sh spark://$MASTER_IP:7077
  # éªŒè¯è¿›ç¨‹æ˜¯å¦å¯åŠ¨
  if jps | grep -q Worker; then
    echo 'âœ… Workerè¿›ç¨‹å¯åŠ¨æˆåŠŸ'
  else
    echo 'âŒ Workerè¿›ç¨‹å¯åŠ¨å¤±è´¥ï¼ŒæŸ¥çœ‹æ—¥å¿—ï¼š\$SPARK_HOME/logs'
    exit 1
  fi
"

echo "===== Spark Workerå®‰è£…åŠå¯åŠ¨å®Œæˆ ====="