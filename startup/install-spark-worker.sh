#!/bin/bash
set -e
exec > /var/log/spark-worker-install.log 2>&1

echo "===== å¼€å§‹å®‰è£…Spark Worker ====="

# 1. å®‰è£…ä¾èµ–
echo "æ­¥éª¤1ï¼šå®‰è£…Javaå’ŒSSH"
apt update -y
apt install -y openjdk-11-jdk openssh-server
systemctl enable --now ssh
echo "Javaç‰ˆæœ¬ï¼š$(java -version 2>&1 | head -1)"

# 2. åˆ›å»ºsparkç”¨æˆ·
echo "æ­¥éª¤2ï¼šåˆ›å»ºsparkç”¨æˆ·"
id spark &>/dev/null || useradd -m -s /bin/bash spark
echo "sparkç”¨æˆ·IDï¼š$(id -u spark)"

# 3. å®‰è£…Spark 3.5.7ï¼ˆæ˜¾ç¤ºè¿›åº¦æ¡ï¼Œç¦ç”¨å®‰é™æ¨¡å¼ï¼‰
echo "æ­¥éª¤3ï¼šå®‰è£…Spark 3.5.7"
SPARK_HOME="/home/spark/spark"
SPARK_TAR="spark-3.5.7-bin-hadoop3.tgz"
SPARK_URL="https://dlcdn.apache.org/spark/spark-3.5.7/$SPARK_TAR"
BACKUP_URL="https://mirrors.aliyun.com/apache/spark/spark-3.5.7/$SPARK_TAR"

if [ ! -d "$SPARK_HOME" ]; then
  su - spark -c "
    cd /home/spark
    echo 'ðŸ“¥ ä»Žä¸»æºä¸‹è½½Sparkï¼ˆæ˜¾ç¤ºè¿›åº¦ï¼‰...'
    if ! wget --show-progress --retry-connrefused --waitretry=3 --read-timeout=30 --timeout=15 -t 5 $SPARK_URL; then
      echo 'âŒ ä¸»æºä¸‹è½½å¤±è´¥ï¼Œåˆ‡æ¢åˆ°å¤‡ç”¨æº...'
      if ! wget --show-progress --retry-connrefused --waitretry=3 --read-timeout=30 --timeout=15 -t 5 $BACKUP_URL; then
        echo 'âŒ å¤‡ç”¨æºä¸‹è½½ä¹Ÿå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿žæŽ¥'
        exit 1
      fi
    fi
    echo 'ï¿½è§£åŽ‹Sparkå®‰è£…åŒ…...'
    if ! tar -xzf $SPARK_TAR; then
      echo 'âŒ è§£åŽ‹å¤±è´¥ï¼Œå®‰è£…åŒ…å¯èƒ½æŸå'
      exit 1
    fi
    mv spark-3.5.7-bin-hadoop3 spark
    rm -f $SPARK_TAR
    echo 'âœ… Sparkå®‰è£…æˆåŠŸ'
  "
else
  echo "âœ… Sparkå·²å®‰è£…ï¼Œè·³è¿‡æ­¤æ­¥éª¤"
fi
chown -R spark:spark /home/spark/spark

# 4. é…ç½®**æ°¸ä¹…ç”Ÿæ•ˆ**çš„çŽ¯å¢ƒå˜é‡ï¼ˆå†™å…¥.bashrcï¼‰
echo "æ­¥éª¤4ï¼šé…ç½®æ°¸ä¹…çŽ¯å¢ƒå˜é‡"
su - spark -c "
  cat >> ~/.bashrc << 'EOF'
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export SPARK_HOME=/home/spark/spark
export PATH=\$PATH:\$SPARK_HOME/bin:\$SPARK_HOME/sbin
EOF
  # å¼ºåˆ¶åŠ è½½çŽ¯å¢ƒå˜é‡ï¼ˆç¡®ä¿å½“å‰ä¼šè¯ç”Ÿæ•ˆï¼‰
  source ~/.bashrc
"

# 5. é…ç½®spark-env.shï¼ˆæ˜¾å¼å¯¼å‡ºæ‰€æœ‰å˜é‡ï¼‰
echo "æ­¥éª¤5ï¼šé…ç½®spark-env.sh"
su - spark -c "
  export SPARK_HOME=/home/spark/spark
  cp \$SPARK_HOME/conf/spark-env.sh.template \$SPARK_HOME/conf/spark-env.sh
  cat >> \$SPARK_HOME/conf/spark-env.sh << 'EOF'
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export SPARK_MASTER_HOST=spark-master
export SPARK_MASTER_PORT=7077
export SPARK_WORKER_MEMORY=2g
export SPARK_WORKER_CORES=1
EOF
"

# 6. å‡†å¤‡SSHç›®å½•ï¼ˆç­‰å¾…Masterå…¬é’¥ï¼‰
echo "æ­¥éª¤6ï¼šåˆå§‹åŒ–SSHç›®å½•"
su - spark -c "
  mkdir -p ~/.ssh
  chmod 700 ~/.ssh
  touch ~/.ssh/authorized_keys
  chmod 600 ~/.ssh/authorized_keys
"

# 7. å¯åŠ¨Spark Workerï¼ˆ**æ˜¾å¼åŠ è½½çŽ¯å¢ƒå˜é‡**ï¼Œç¡®ä¿è·¯å¾„æ­£ç¡®ï¼‰
echo "æ­¥éª¤7ï¼šå¯åŠ¨Spark Worker"
su - spark -c "
  # å¼ºåˆ¶åŠ è½½.bashrcä¸­çš„çŽ¯å¢ƒå˜é‡
  source ~/.bashrc
  # èŽ·å–Master IPï¼ˆä¾èµ–GCPå†…éƒ¨DNSè§£æžï¼‰
  MASTER_IP=\$(nslookup spark-master | grep 'Address: ' | tail -n 1 | awk '{print \$2}')
  if [ -z \"\$MASTER_IP\" ]; then
    echo 'âŒ æ— æ³•è§£æžspark-masterçš„IPï¼Œæ‰‹åŠ¨æŒ‡å®šMaster IPåŽé‡è¯•'
    exit 1
  fi
  echo 'ðŸ”— è¿žæŽ¥åˆ°MasterèŠ‚ç‚¹ï¼š\$MASTER_IP:7077'
  \$SPARK_HOME/sbin/start-worker.sh spark://\$MASTER_IP:7077
  echo 'âœ… Spark Workerå¯åŠ¨å‘½ä»¤å·²æ‰§è¡Œï¼Œè¿›ç¨‹çŠ¶æ€å¯é€šè¿‡jpsæˆ–æ—¥å¿—æ£€æŸ¥'
"

echo "===== Spark Workerå®‰è£…åŠå¯åŠ¨å®Œæˆ ====="